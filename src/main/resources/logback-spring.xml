<?xml version="1.0" encoding="UTF-8"?>
<configuration scan="true" scanPeriod="60 seconds" debug="true">
    <!-- 定义日志文件的存储地址 -->
    <property name="LOG_HOME" value="/data/springcloud/logs/logback"/>
    <springProperty scope="context" name="appName" source="spring.application.name"/>
    <springProperty scope="context" name="ip" source="spring.cloud.client.ipAddress"/>

    <springProperty scope="context" name="kafkaServers" source="kafka.servers"/>

    <!-- 上下文名称 -->
    <contextName>${LOG_PROJECT}</contextName>


    <!-- 控制台输出 -->
    <appender name="console" class="ch.qos.logback.core.ConsoleAppender">
        <encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
            <!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度，%msg：日志消息，%n是换行符-->
            <pattern>%d{yyyy-MM-dd HH:mm:ss.SSS} [%thread] %-5level %logger{100} - %msg%n</pattern>
        </encoder>

        <!-- 输出warn和error信息 -->
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>DEBUG</level>
        </filter>
    </appender>

    <!-- kafka配置 -->
    <appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
        <topic>pe-app-logs</topic>
        <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.HostNameKeyingStrategy"/>
        <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy"/>
        <appendTimestamp>true</appendTimestamp>
        <producerConfig>bootstrap.servers=${kafkaServers}</producerConfig>
        <!--将缓冲批次的大小限制为8MB（默认为32MB）-->
        <producerConfig>buffer.memory=8388608</producerConfig>
        <encoder class="net.logstash.logback.encoder.LogstashEncoder">
            <providers>
                <timestamp>
                    <timeZone>UTC</timeZone>
                </timestamp>
                <pattern>
                    <pattern>
                        {
                        "ip": "${ip}",
                        "app": "${appName}",
                        "level": "%level",
                        "trace": "%X{X-B3-TraceId:-}",
                        "span": "%X{X-B3-SpanId:-}",
                        "parent": "%X{X-B3-ParentSpanId:-}",
                        "thread": "%thread",
                        "class": "%logger{40}",
                        "message": "%message",
                        "stack_trace": "%exception"
                        }
                    </pattern>
                </pattern>
            </providers>
        </encoder>
        <!--  将无法发送到kafka的每条日志消息记录到console -->
        <appender-ref ref="console"/>
    </appender>

    <logger name="org.apache.kafka" level="WARN"/>

    <springProfile name="local">
        <root level="INFO">
            <appender-ref ref="console"/>
        </root>
    </springProfile>
    <springProfile name="pro,dev,test">
        <root level="INFO">
            <appender-ref ref="console"/>
            <appender-ref ref="kafkaAppender-pro"/>
        </root>
    </springProfile>


</configuration>
